Sort alt masking - gets an output with all masks one way but with alternating

 currently images are black due to nans and infs
 channel mask last layer resnet output seems to get too large so exp overflows
 but channel masking seems ok
 so issue in weights or inputs
 - could be to do with initialisation or data preprocess

 Looking at the two channel masks separately (ie. layer group 1 only or layer group 2 only:
 Checkerboard on it's own
    Seems OK with all masks alt, but black outputs and nan loss if alt masks
    Ok with 2 or 3 alt layers
    But with 4 alt layers after one or two updates the resnet values blow up and cause issue described
    Changing resnet doesn't seem to fix, but resnet seems to be issue leading on to overflow
    - Resnet?
    - Post-resnet?
    - Input to resnet?

Looked at solutions here, adding tanh to log_scale outputs helped with nan, still needs work though!

layer group 1 on its own works on debug dataset now and on faces
layer group 2 on its own works on debug dataset now

So merging them is an issue! Doesn't work as well as layer group 1 on own though does get some of the way
Doesn't work with merged (all 3 layer groups) on celeb faces

Layer 1 and layer 3 seems to work
Doesn't work layer 1 squeeze + un and layer 3
For layer 1 + layer 3 both un/squeeze type 1 and 2 don't work

Extensive testing on squeeze / unsqueeze it matches solution's

# TODO: samples bad, interp ok
    *** compare affine channel outs to same ins and review code - seems to be different in current tests
    affine channel simpleresnet should be n_out * 2?
    .
    why initial loss much higher on mine?
    Try longer training run
    Conv padding?
    Try higher LR
sample zs mean 0, std 1
interp was more like mean -0.2, std 0.8

tried adding preprocess loss - not solved it

tried both forms of channel masking (mask or split inputs)

ran solution code for 5 epochs and samples poor, interp ok so could be training poorly?
Ran mine for 100 iters with same train code and hyperparams as solns - still bad samples, but loss goes down much slower too

comparing masking
both maskings are the same but with inverted "type" argument ie. type=1.0 in solution is False in mine

log scale init in act norm is the same

samples different but not there after changes
    invert masking argument
    preprecoess inverse should be [0, 1] so not the scale by n part

samples not changed
    changed channel layer init to zeros (think all param inits are the same now re. initial loss being higher than solutions)
    print mean and std for z in training for mine and solution code
    also changed clip norm to 0.5

checking class by class reviewed code compared to mine visually (ordered by solution file)
    WeightNormConv2d
    ActNorm
    ResNetBlock
    SimpleRes
    RealNVP
    Solver
Comparing outputs of affine coupling classes (taking out the resnet parts and ensuring same inits)
Affine checkerboard the same!


# TODO: warning about WeightNorm retrace - interp?